{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9fce7e9",
   "metadata": {},
   "source": [
    "- 38:06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b08db3",
   "metadata": {},
   "source": [
    "# Roadmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d22ed",
   "metadata": {},
   "source": [
    "- linux\n",
    "- sql\n",
    "- java /python/scala\n",
    "- bigdata  ( hdfs,hive->(hadoop), batch,spark sql->(spark) )\n",
    "- ETL concepts\n",
    "- cloud\n",
    "- work on projects, do optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c207e28",
   "metadata": {},
   "source": [
    "### File system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a73693",
   "metadata": {},
   "source": [
    "- windows (NTFS - new technology file system )\n",
    "- linux (EXT-extensible file system)\n",
    "- mac (macfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d775c2",
   "metadata": {},
   "source": [
    "- if read/write data , it goes through file system from/to  hard disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b870ca0a",
   "metadata": {},
   "source": [
    "- NTFS,EXT --> Standalone file system\n",
    "- HDFS,S3  --> Distributed file system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fff18d",
   "metadata": {},
   "source": [
    "- Block \n",
    "  - if we want to store a 1 gb data , it will be split it blocks and store\n",
    "  - NTFS - 16 KB\n",
    "  - EXT  - 512 kb\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51795119",
   "metadata": {},
   "source": [
    "1:03:40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d2d46",
   "metadata": {},
   "source": [
    "- master slave (hadoop,spark)\n",
    "- peer to peer (cassandra)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf64fa",
   "metadata": {},
   "source": [
    "- SPOF (Single point of failure)\n",
    "  - reason is SPOC(Single point of communication)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0c490",
   "metadata": {},
   "source": [
    "- a programm in excecution is process\n",
    "- a background process - daemon process\n",
    "-  hadoop with 5 daemon process (jp1,jp2,jp3,jp4,jp5)(JAVA PROCESS)\n",
    "   - first 3 for hdfs ,next 2 for mapreduce\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8dade",
   "metadata": {},
   "source": [
    "- hadoop hdfs have default replication technology(3 replicas)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e268f7",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005e92b",
   "metadata": {},
   "source": [
    " ![](master_to_process_jp1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2849d746",
   "metadata": {},
   "source": [
    "- all slaves run(JP2)\n",
    "- the node which run JP1 hadoop process is master\n",
    "- the node which run JP2 hadoop process is slave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef04ee3",
   "metadata": {},
   "source": [
    "- how to connect to cluster to read and write\n",
    "- in real time they will not allow you to directly connect to node \n",
    "   - they will create a firewall kind of node **(Edge node)**\n",
    "\n",
    "- if you are creating multinode cluster on your own then can connect slave machine without edge node concept\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00a48b0",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
